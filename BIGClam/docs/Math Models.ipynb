{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Математические модели и формулы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>TODO:</font> Заменить скобки на нормальные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обозначения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $G = (V,E) $ — граф\n",
    "* $A \\in \\mathbb{R}^{N\\times N}$ — матрица смежности \n",
    "* $F \\in \\mathbb{R}^{N\\times K}$ — матрица принадлежности к сообществам\n",
    "* $K$ — количество сообществ\n",
    "* $C$ — множество сообществ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BigCLAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/BigCLAM_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предположения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Каждая вершина $v$ относится к сообществу $c$ с некоторой силой $F_{vc} >0$. \n",
    "* Вероятность появления ребра $(u,v)$, при условии, что вершины $u,v$ находятся в одном сообществе $c$ есть \n",
    "\n",
    "$$P((u,v) | c)=1 - \\exp(-F_{uc} F_{vc}).$$\n",
    "\n",
    "* Каждое сообщество $c$ генерирует ребра независимо друг от друга, а значит, что вероятность появления ребра \n",
    "\n",
    "$$P(u,v)=1 - \\exp(-\\sum_{c\\in C} F_{uc} F_{vc}) = 1 - \\exp( - F_{u} F_{v}^T),$$\n",
    "\n",
    "$$F = \\{F_u\\} = \\{F_{uc}\\} \\in \\mathbb{R}^{N \\times K}. $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вероятностная интерпретация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Каждые две вершины $u, v$ взаимодействуют с некоторой силой $X_{uv}$. Чем больше сила, тем больше вероятность $p(u,v)$ появления ребра.\n",
    "* Сила взаимодействия вершин $u, v$ определяется сообществами. Каждое сообщество $c$, в которое входит одновременно 2 вершины дает свой аддитивный вклад в силу их взаимодействия $X_{uv}^{(c)}$.\n",
    "* **Предполагаем**, что $X_{uv}^{(c)} \\sim \\mathrm{Pois}(F_{uc} \\cdot F_{vc})$, где $F_{vc}>0$ сила взаимодействия вершины $v$ и сообщества $c$. Значит, что \n",
    "\n",
    "$$X_{uv} \\sim \\mathrm{Pois}(\\sum_{c} F_{uc} \\cdot F_{vc}) = \\mathrm{Pois}(F_{u} \\cdot F_{v}^T).$$\n",
    "\n",
    "* **Предполагаем**, что ребро появляется, если $X_{uv} > 0$. Т.е.\n",
    "\n",
    "$$p(u,v) = \\mathbb{P}(X_{uv} > 0) = 1 - \\exp( - F_{u} F_{v}^T).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\varepsilon$-сообщество\n",
    "Предполагаем, что все вершины относятся к большому $\\varepsilon$-сообществу с малой силой ($\\approx 10^{-6}$) (т.к. иначе, вершины, не входящие в одно сообщество не могут быть соединены ребром). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используется метод максимизации правдоподобия.\n",
    "\n",
    "\\begin{align}\n",
    "    l(F) & = log(\\mathbb{P}(A|F))\\\\\n",
    "    & = \\sum_{(u,v)\\in E} \\log(1 - \\exp( - F_{u} F_{v}^T)) - \\sum_{(u,v) \\notin E} F_u F_v^T.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Схема оптимизации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такая задача является частным случаем NMF (Non-negative matrix factorization):\n",
    "Ищем такую низкоранговую матрицу $F\\in \\mathbb{R}^{N \\times K}$, что она наилучшим образом приближает матрицу $A$ в смысле правдоподобия (на самом деле $l_2$-норма плохо подходит для восстановления бинарных матриц): \n",
    "\n",
    "$$ \\hat{F} = \\arg \\min_{F \\ge 0} D(A, f(FF^T)),$$\n",
    "\n",
    "$$\\text{гдe}\\quad D = -l(F), \\quad f(x) = 1-\\exp(-x).$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оптимизации используется блочный координатный спуск с методом проекции градиента на каждом шаге.\n",
    "Фиксируем $F_v$, оптимизируем по $F_u$, $u \\ne v$. Задача становится выпуклой.\n",
    "\n",
    "$$\\forall u: \\quad \\arg\\max_{F_u \\ge 0} l(F_u), $$\n",
    "\n",
    "$$l(F_u) = \\sum_{v \\in \\mathcal{N}(u)} \\log(1-\\exp(-F_u F_v^T)) - \\sum_{v \\notin \\mathcal{N}(u)} F_u F_v^T, $$\n",
    "\n",
    "где $\\mathcal{N}(u)$ — соседи вершины u.\n",
    "\n",
    "$$\\nabla l(F_u) = \\sum_{v \\in \\mathcal{N}(u)} F_u \\dfrac{\\exp(-F_u F_v^T)}{1-\\exp(-F_u F_v^T)} - \\sum_{v \\notin \\mathcal{N}(u)} F_v^T. $$\n",
    "\n",
    "Основная сложность формулы (линейная по размеру графа) во втором слагаемом. Заметим, что \n",
    "\n",
    "$$\\sum_{v \\notin \\mathcal{N}(u)} F_v^T = \\sum_v{F_v} - F_u - \\sum_{v\\in \\mathcal{N}(u)} F_v.$$ \n",
    "\n",
    "Получаем сложность одной итерации $O(\\mathcal{N}(u))$.\n",
    "\n",
    "Для подбора градиентного шага используем backtracking line search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Восстановление структуры сообществ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы восстановить исходную структуру сообществ $C$ сравним значение матрицы $F$ с порогом $\\delta$. Обозначим за $\\varepsilon$ вероятность появления ребра в графе (если бы все ребра появлялись равномерно): $\\varepsilon = \\dfrac{2|V|}{|E|\\cdot (|E|-1)}$. Возьмем $\\delta$ так, чтобы две вершины принадлежали одному сообществу, если модельная вероятность появления ребра между ними выше чем $\\varepsilon$:\n",
    "\n",
    "$$\\varepsilon \\le 1-\\exp(-\\delta^2)$$\n",
    "\n",
    "$$\\delta = \\sqrt{-\\log(1-\\varepsilon)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>TODO:</font> Проверить. Возможно, порог стоит сделать немного больше (~ статистически значимое отклонение )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>TODO:</font> Заполнить"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WeiCLAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самое простое изменение BigCLAM для обработки взвешенных ребер:\n",
    "\n",
    "$$l(F) = \\sum_{(u,v)\\in E} \\log(1 - \\exp\\left( - \\dfrac{F_{u} F_{v}^T}{w_{uv}}\\right) - \\sum_{(u,v) \\notin E} \\dfrac{F_{u} F_{v}^T}{w_{uv}}.$$\n",
    "\n",
    "Тем самым, мы получаем, что чем больше вес $w_{uv}$, тем больше должно быть значение сил $F_u$ и $F_v$, которые его объясняют.\n",
    "\n",
    "Т.е. Предположение, что вероятность появления ребра $(u,v)$, при условии, что вершины $u,v$ находятся в одном сообществе $c$ есть \n",
    "\n",
    "$$P((u,v) | c)=1 - \\exp\\left( -F_{uc} F_{vc} \\right).$$\n",
    "\n",
    "Заменяется на предположение, что вероятность появления ребра $(u,v)$ **с весом $w_{uv}$**, при условии, что вершины $u,v$ находятся в одном сообществе $c$ есть \n",
    "\n",
    "$$P((u,v) | c, w_{uv})=1 - \\exp\\left(-\\dfrac{F_{uc} F_{vc}}{w_{uv}}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>TODO:</font> Дописать.\n",
    "\n",
    "Красивая вероятностная интерпретация: $$X_{uv}^{(c)} \\sim \\mathrm{Pois}\\left(\\dfrac{F_{uc} \\cdot F_{vc}}{w_{uv}}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тесты подтверждают работоспособность модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GammaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Совершенно другая вероятностная модель к описанию наблюдаемых графов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предпосылки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "тут еще есть про \"правильный\" аналог непрерывного распределения Пуассона: http://ac.inf.elte.hu/Vol_039_2013/137_39.pdf\n",
    "надо попробовать потренировать ее тоже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы посмотрим на первоначальную модель, то увидим, что в ней есть скрытые переменные $X_{uv}$, которые распределены по Пуассону. Данное распределение является дискретным, а веса на ребрах -- непрерывные. \n",
    "\n",
    "Самое главное свойство, которые использовались в выводе — мультипликативность. Так что в качестве непрерывного аналога распределения Пуассона можно взять Гамма распределение, сумма которых (с одинаковым коэффициентом масштаба) не выводит из класса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предположения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Вероятность появления ребра с весом $w_{uv}$, при условии, что вершины принадлежат сообществу $c$\n",
    "\n",
    "$$p\\left(w_{uv} | c\\right) \\sim \\mathrm{\\Gamma}\\left(k=F_u F_v^T + 1, \\theta=1\\right).$$\n",
    "\n",
    "* Каждое сообщество $c$ генерирует ребра независимо друг от друга, а значит, что вес ребра\n",
    "\n",
    "$$w_{uv} = \\sum_{c} w_{uv}^c \\sim \\mathrm{\\Gamma}\\left(\\sum_c F_{uc} F_{vc} + 1, 1\\right) = \\mathrm{\\Gamma}\\left(F_u F_v^T + 1, 1\\right).$$\n",
    "\n",
    "Берем $+ 1$ для того, чтобы не сталкиваться с распределениями с бесконечной плотностью в нуле. В вероятностном смысле это безусловная (от сообществ) вероятность появления ребра в графе.\n",
    "\n",
    "\n",
    "Для простоты везде далее будем опускать параметр $\\theta$.\n",
    "\n",
    "Обозначим $K_{uv} = F_u F_v^T + 1.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "    l\\left(F\\right) & = log\\left(\\mathbb{P}\\left(A|F\\right)\\right) = \\sum_{w_{uv}} \\log p(w_{uv}) \\\\\n",
    "    & = \\sum_{w_{uv}}\\left[-\\log\\mathrm{\\Gamma}\\left(K_{uv}\\right) - K_{uv}\\log\\theta + (K_{uv} - 1)\\cdot\\log w_{uv} - \\dfrac{w_{uv}}{\\theta}\\right] \\\\\n",
    "    & = \\sum_{w_{uv}}\\left[-\\log\\mathrm{\\Gamma}\\left(K_{uv}\\right) + (K_{uv} - 1)\\cdot\\log w_{uv} - w_{uv}\\right] \\\\\n",
    "    & = \\sum_{w_{uv}}\\left[-\\log\\mathrm{\\Gamma}\\left( F_u F_v^T + 1 \\right) + F_u F_v^T \\cdot\\log w_{uv} - w_{uv}\\right] \\rightarrow \\max_{F\\ge 0}. \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Схема оптимизации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы посчитать градиент, нам понадобится дигамма функция:\n",
    "\n",
    "$$\\Psi(x) = \\dfrac{\\mathrm{d}}{\\mathrm{d}x} \\log\\left(\\mathrm\\Gamma(x)\\right).$$\n",
    "\n",
    "Тогда градиент можем записать как\n",
    "$$\\dfrac{\\mathrm{d}l(F)}{\\mathrm{d}F_u} = - \\sum_v F_v \\Psi\\left(F_u F_v^T + 1\\right) + F_v \\cdot  \\left( \\log\\theta - \\log w_{uv}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Схема оптимизации используется та же самая, что и в BigCLAM.\n",
    "\n",
    "Ко всем весам прибавляется небольшое $\\varepsilon$, чтобы избежать нулей под логарифмом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для ускорения вычисления можно отдельно хранить $\\log w_{uv}$ и $\\Psi\\left(F_u F_v^T + 1\\right)$, каждый раз обновляя 1 строчку и 1 столбец. Тем не менее, т.к. сумма взвешенная, провести такой трюк как в BigCLAM не получится. Для каждого шага, для каждого $F_u$ придется пересчитывать сумму целиком. Получается линейная сложность. Поэтому для подбора шара нецелесообразно использовать backtraking. Использовался обычный убывающий шаг."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Результаты экспериментов на модельных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "В ходе экспериментов было рассмотрено 2 варианта моделирования матрицы смежности взвешенного графа. 1 модель была взята прямо из предположений, описанных выше: задается матрица $F$, веса генерируются из гамма распределения. На таких данных оптимизационная схема надежно работает и сходится из любого, даже случайного приближения.\n",
    "\n",
    "Однако, такая модель данных не соответствует реальности, т.к. в настоящие социальные графы разреженные. Вторая модель данных учитывала этот факт. Сначала генерировалась структура графа (есть ребро или нет), затем, только для проявившихся ребер генерируется его вес.\n",
    "\n",
    "* Задается матрица $F$.\n",
    "* $\\forall\\, u \\in V,v \\in V$ С вероятностью $1 - \\exp(-\\gamma F_u {F_v}^T)$ в графе создается ребро $(u, v) \\in E$.\n",
    "* $\\forall\\, (u, v) \\in E$ — созданных ребер генерируется вес $w_{uv} \\sim \\mathrm{\\Gamma}\\left(\\sum_c F_{uc} F_{vc} + 1, 1\\right)$.\n",
    "\n",
    "$\\gamma$ дополнительный параметр модели. Чем меньше $\\gamma$, тем более разреженной является результирующая матрица смежности $A$.\n",
    "\n",
    "Оказывается, что предложенная Гамма модель не может объяснить большое количество нулей в подобного рода данных и оптимизация не приводит ни к какому адекватному результату. Необходимо дополнительно учитывать возникающие в данных нули. Для этого была предложена следующая модель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Gamma Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предпосылки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Описаны пунктом выше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предположения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все предположения о природе данных возьмем из предложенной процедуры генерации данных. Т.е. будем предполагать, что настоящие данные генерируются из такого рода модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С этого момента будем обозначать вес ребра за $w_{uv}$, а бинаризованную матрицу смежности за $a_{uv}$. Т.е. $a_{uv} = \\mathbb I \\left[w_{uv} \\ne 0\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, что т.к. вес ребра $w_{uv}$ отличен от 0 только если $a_{uv}\\ne0$, \n",
    "\n",
    "$$ p(w_{uv}=0 | a_{uv}=0) = 1.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, выведем формулу логарифма правдоподобия.\n",
    "\n",
    "\\begin{align}\n",
    "    l(F) & = \\sum_{\\forall\\,(u,v)} \\log p(w_{uv}) = \\sum_{u,v} \\log p(w_{uv} | a_{uv}) + \\log p(a_{uv}) \\\\ \n",
    "    & = \\sum_{(u,v)\\in E} \\log p(w_{uv} | a_{uv}=1) + \\log p(a_{uv}=1) + \\sum_{(u,v)\\notin E} \\log p(w_{uv}=0 | a_{uv}=0) + \\log p(a_{uv}=0) \\\\\n",
    "    & = \\sum_{(u,v)\\in E} \\log p(w_{uv} | a_{uv}=1) + \\log p(a_{uv}=1) + \\sum_{(u,v)\\notin E} \\log p(a_{uv}=0) \\\\\n",
    "    & = \\sum_{(u,v)\\in E} \\log \\mathrm{P_\\Gamma}(w_{uv}) + \\sum_{(u,v)\\in E} \\log\\left(1-\\exp\\left(-\\gamma F_u {F_v}^T\\right)\\right) - \\gamma \\sum_{(u,v)\\notin E} F_u {F_v}^T.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первое слагаемое — это правдоподобие Гамма модели на ребрах, а последние 2 слагаемых это BigClam модель для матрицы $\\sqrt \\gamma F$! \n",
    "Значит, получившаяся модель является их комбинацией. Модель сохраняет все преимущества BigClam-модели, но учитывает взвешенные ребра и имеет дополнительный параметр $\\gamma$. Отметим, что этот параметр может автоматически подбираться из степени разреженности матрицы смежности\n",
    "\n",
    "<font color='red'>TODO:</font> Понять как именно получить оценку на $\\gamma$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Схема оптимизации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данный момент используется уменьшающийся шаг, однако, планируется использовать backtracking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель отлично справляется со малым количеством ребер и восстанавливает матрицу $F$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
